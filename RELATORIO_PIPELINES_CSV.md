# ğŸ“Š RelatÃ³rio de ImplementaÃ§Ã£o - Pipelines CSV (02 e 04)
## SoluÃ§Ãµes Implementadas e PadrÃµes para ReplicaÃ§Ã£o

---

## âœ… PIPELINE 02 - Desvio de Carregamento

### ğŸ¯ Funcionalidades Implementadas

1. **Processamento de CSV com cabeÃ§alho na 2Âª linha**
2. **Filtro de vagÃµes especÃ­ficos (BAHMAN e SILOKING)**
3. **CÃ¡lculo automÃ¡tico de status baseado em desvio percentual**
4. **InserÃ§Ã£o em lote de 500 registros**

### ğŸ“ Estrutura da Tabela
```sql
staging_02_desvio_carregamento:
- organization_id (UUID)
- file_id (UUID)
- data (TEXT) - formato yyyy-MM-dd
- hora (TEXT)
- pazeiro (TEXT)
- vagao (TEXT)
- dieta (TEXT)
- nro_carregamento (TEXT)
- ingrediente (TEXT)
- tipo_ingrediente (TEXT)
- realizado_kg (NUMERIC)
- previsto_kg (NUMERIC)
- desvio_kg (NUMERIC)
- desvio_pc (NUMERIC)
- status (TEXT) - VERDE/AMARELO/VERMELHO
- merge (TEXT) - chave Ãºnica
```

---

## âœ… PIPELINE 04 - Itens de Trato

### ğŸ¯ Funcionalidades Implementadas

1. **Processamento de CSV com cabeÃ§alho na 2Âª linha**
2. **Filtro de vagÃµes especÃ­ficos (BAHMAN e SILOKING)**
3. **RemoÃ§Ã£o automÃ¡tica de linhas de total/agrupamento**
4. **InserÃ§Ã£o em lote de 500 registros**

### ğŸ“ Estrutura da Tabela
```sql
staging_04_itens_trato:
- organization_id (UUID)
- file_id (UUID)
- data (TEXT) - formato yyyy-MM-dd
- id_carregamento_original (TEXT)
- hora (TEXT)
- dieta (TEXT)
- carregamento (TEXT)
- ingrediente (TEXT)
- realizado_kg (NUMERIC)
- pazeiro (TEXT)
- vagao (TEXT)
- ms_dieta_pc (NUMERIC)
- ndt_dieta_pc (NUMERIC)
- merge (TEXT) - chave Ãºnica
```

---

## âœ… PIPELINE 05 - Trato por Curral

### ğŸ¯ Funcionalidades Implementadas

1. **Processamento de CSV com cabeÃ§alho na 2Âª linha**
2. **RemoÃ§Ã£o automÃ¡tica de linhas de total/agrupamento**
3. **Tratamento especial para colunas vazias**
4. **Parsing manual para controle total das posiÃ§Ãµes**

### ğŸ“ Estrutura da Tabela
```sql
staging_05_trato_por_curral:
- organization_id (UUID)
- file_id (UUID)
- data (DATE) - formato yyyy-MM-dd
- hora (TIME)
- vagao (VARCHAR)
- curral (VARCHAR)
- id_carregamento (VARCHAR)
- lote (VARCHAR)
- trato (VARCHAR)
- realizado_kg (DECIMAL)
- dieta (VARCHAR)
- tratador (VARCHAR)
- ms_dieta_pc (DECIMAL)
- merge (VARCHAR) - chave Ãºnica: data-hora-vagao-trato
```

### ğŸš› Problemas EspecÃ­ficos Resolvidos

#### âŒ Problema: Coluna VagÃ£o vazia
**Causa:** Colunas vazias no CSV causavam desalinhamento no parsing
**SoluÃ§Ã£o:** ImplementaÃ§Ã£o de parsing manual seguindo padrÃµes estabelecidos:
- CabeÃ§alho na linha 2
- RemoÃ§Ã£o de linhas agregadoras
- MÃºltiplas variaÃ§Ãµes de header (VagÃ£o/Vagï¿½o/Vagao)

#### âŒ Problema: Incompatibilidade de merge entre staging 03 e 05
**Causa:** Staging 03 tinha "Trato 1", "Trato 2" vs Staging 05 tinha "1", "2"
**SoluÃ§Ã£o:** ModificaÃ§Ã£o da staging 03 para extrair apenas nÃºmero no merge:
```typescript
// Extract only the number from trato for merge compatibility with staging 05
// "Trato 1" -> "1", "Trato 2" -> "2", etc.
const tratoNumber = trato.replace(/^Trato\s*/i, '').trim() || trato;

// Generate merge field with trato NUMBER
const merge = `${dataFormatted}-${hora}-${vagao}-${tratoNumber}`;
```
**Resultado:** Campos merge compatÃ­veis entre staging 03 e 05 mantendo dados originais intactos

---

## ğŸ”§ PADRÃ•ES DE IMPLEMENTAÃ‡ÃƒO PARA REPLICAR

### 1ï¸âƒ£ **Tratamento de CSV Brasileiro**

#### âœ… CabeÃ§alho na 2Âª Linha
```typescript
// Parse sem header primeiro
const { data: allLines } = Papa.parse(csvText, {
  delimiter: ";",
  skipEmptyLines: true
});

// Validar mÃ­nimo de linhas
const lines = allLines as string[][];
if (lines.length < 2) {
  throw new Error('Arquivo CSV deve ter pelo menos 2 linhas');
}

// Usar 2Âª linha como header
const headers = lines[1].map((header: string) => {
  return header
    .replace('Inclusï¿½o', 'InclusÃ£o')
    .replace('Vagï¿½o', 'VagÃ£o')
    .trim();
});

// Pular primeiras 2 linhas
const dataLines = lines.slice(2);
```

#### âœ… RemoÃ§Ã£o de Linhas de Total/Agrupamento
```typescript
// Remover Ãºltimas linhas que sÃ£o totais
while (dataLines.length > 0) {
  const lastLine = dataLines[dataLines.length - 1];
  const firstColumn = (lastLine[0] || '').trim().toLowerCase();

  if (
    firstColumn.includes('total') ||
    firstColumn.match(/^[a-zA-Z]{3}\/\d{2}$/) || // jan/25
    firstColumn === '' ||
    lastLine.every(cell => (cell || '').trim() === '')
  ) {
    dataLines.pop();
  } else {
    break;
  }
}
```

### 2ï¸âƒ£ **ConversÃ£o de Dados**

#### âœ… Datas Brasileiras â†’ ISO (yyyy-MM-dd)
```typescript
function parseDate(dateString: string | undefined): string {
  if (!dateString || dateString.trim() === '') return '';

  const cleaned = dateString.trim();

  // Skip datas invÃ¡lidas
  if (cleaned.toLowerCase().includes('total') ||
      cleaned.match(/^[a-zA-Z]{3}\/\d{2}$/)) {
    return '';
  }

  // Patterns brasileiros
  const patterns = [
    /^(\d{1,2})\/(\d{1,2})\/(\d{4})$/, // dd/MM/yyyy
    /^(\d{1,2})-(\d{1,2})-(\d{4})$/,   // dd-MM-yyyy
    /^(\d{1,2})\.(\d{1,2})\.(\d{4})$/  // dd.MM.yyyy
  ];

  for (const pattern of patterns) {
    const match = cleaned.match(pattern);
    if (match) {
      const day = match[1].padStart(2, '0');
      const month = match[2].padStart(2, '0');
      const year = match[3];

      // Validar ranges
      const dayNum = parseInt(day);
      const monthNum = parseInt(month);
      const yearNum = parseInt(year);

      if (dayNum >= 1 && dayNum <= 31 &&
          monthNum >= 1 && monthNum <= 12 &&
          yearNum >= 1900 && yearNum <= 2100) {
        return `${year}-${month}-${day}`;
      }
    }
  }

  return '';
}
```

#### âœ… NÃºmeros Brasileiros â†’ Decimal
```typescript
function parseNumericValue(value: string | undefined): number {
  if (!value || value.trim() === '') return 0;

  let cleaned = value.replace(/%/g, '').trim();

  // Negativos
  const isNegative = cleaned.startsWith('-');
  if (isNegative) {
    cleaned = cleaned.substring(1);
  }

  // Formato brasileiro
  if (cleaned.includes('.') && cleaned.includes(',')) {
    // 1.234.567,89 â†’ 1234567.89
    cleaned = cleaned.replace(/\./g, '').replace(',', '.');
  } else if (cleaned.includes(',') && !cleaned.includes('.')) {
    // 1234,89 â†’ 1234.89
    cleaned = cleaned.replace(',', '.');
  } else if (cleaned.includes('.')) {
    // Detectar se Ã© milhar ou decimal
    const parts = cleaned.split('.');
    if (parts.length === 2 && parts[1].length <= 2) {
      // Decimal (123.45)
    } else if (parts.length > 2 || (parts.length === 2 && parts[1].length > 2)) {
      // Milhares (1.234.567 ou 1.234)
      cleaned = cleaned.replace(/\./g, '');
    }
  }

  const parsed = parseFloat(cleaned);
  const result = isNaN(parsed) ? 0 : parsed;

  return isNegative ? -result : result;
}
```

### 3ï¸âƒ£ **Filtros e ValidaÃ§Ãµes**

#### âœ… Filtro de VagÃµes EspecÃ­ficos
```typescript
// Obter vagÃ£o com mÃºltiplas variaÃ§Ãµes de header
const vagao = (
  row["VagÃ£o"] ||
  row["Vagï¿½o"] ||
  row["Vagao"] ||
  ''
).toUpperCase().trim();

// Filtrar apenas BAHMAN e SILOKING
if (!vagao || (!vagao.includes('BAHMAN') && !vagao.includes('SILOKING'))) {
  console.log(`â­ï¸ Pulando vagÃ£o: "${vagao}"`);
  continue;
}
```

#### âœ… ValidaÃ§Ã£o de Campos ObrigatÃ³rios
```typescript
// Skip linhas vazias ou de agrupamento
const dataInclusao = (row["Data de InclusÃ£o"] || '').trim();
const hora = (row["Hora"] || '').trim();
const motorista = (row["Motorista"] || '').trim();

if (!dataInclusao || !hora || !motorista) {
  console.log(`â­ï¸ Pulando linha vazia/incompleta`);
  continue;
}

// Skip linhas de total
if (dataInclusao.toLowerCase().includes('total') ||
    dataInclusao.match(/^[a-zA-Z]{3}\/\d{2}$/)) {
  console.log(`â­ï¸ Pulando linha de total`);
  continue;
}
```

### 4ï¸âƒ£ **InserÃ§Ã£o em Banco**

#### âœ… InserÃ§Ã£o em Lote com Tratamento de Erro
```typescript
async function insertInBatches(rows: any[], batchSize = 500) {
  const results = {
    success: 0,
    errors: []
  };

  for (let i = 0; i < rows.length; i += batchSize) {
    const batch = rows.slice(i, i + batchSize);

    try {
      const { error } = await supabase
        .from('staging_04_itens_trato')
        .insert(batch);

      if (error) {
        results.errors.push({
          batch: Math.floor(i / batchSize) + 1,
          error: error.message,
          rowsAffected: batch.length
        });
      } else {
        results.success += batch.length;
      }
    } catch (error) {
      results.errors.push({
        batch: Math.floor(i / batchSize) + 1,
        error: error instanceof Error ? error.message : 'Erro desconhecido',
        rowsAffected: batch.length
      });
    }
  }

  return results;
}
```

### 5ï¸âƒ£ **Estrutura da Edge Function**

#### âœ… Template Base
```typescript
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";
import Papa from "https://esm.sh/papaparse@5.4.1";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'GET, POST, OPTIONS'
};

const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
const supabase = createClient(supabaseUrl, supabaseKey);

serve(async (req) => {
  // Handle CORS
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { filename, fileId, organizationId } = await req.json();

    // Validate parameters
    if (!filename || !organizationId) {
      throw new Error('ParÃ¢metros obrigatÃ³rios ausentes');
    }

    const actualFileId = fileId || crypto.randomUUID();

    // Download CSV from storage
    const filePath = `${organizationId}/csv-processed/XX/${filename}`;
    const { data: csvFile, error: downloadError } = await supabase.storage
      .from('csv-uploads')
      .download(filePath);

    if (downloadError) {
      throw new Error(`Falha ao baixar arquivo: ${downloadError.message}`);
    }

    const csvText = await csvFile.text();

    // Process CSV
    const processedData = await processarXX(csvText, organizationId, actualFileId);

    // Insert data
    const insertResults = await insertInBatches(processedData);

    return new Response(JSON.stringify({
      success: true,
      filename,
      fileId: actualFileId,
      rowsProcessed: processedData.length,
      rowsInserted: insertResults.success,
      errors: insertResults.errors.length > 0 ? insertResults.errors : undefined,
      message: `Pipeline XX processado: ${insertResults.success} linhas inseridas`
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });

  } catch (error) {
    return new Response(JSON.stringify({
      success: false,
      error: error instanceof Error ? error.message : 'Erro desconhecido'
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  }
});
```

---

## ğŸ›¡ï¸ PROTEÃ‡Ã•ES CONTRA DUPLICAÃ‡ÃƒO

### ğŸ“‹ ParÃ¢metros da Edge Function
```typescript
const { filename, fileId, organizationId, forceOverwrite = false } = await req.json();
```

### ğŸ”’ VerificaÃ§Ã£o 1: File ID Duplicado
```typescript
// Check if file was already processed
console.log(`ğŸ” Verificando se arquivo jÃ¡ foi processado...`);
const { data: existingData, error: checkError } = await supabase
  .from('staging_XX_tabela')
  .select('id, created_at')
  .eq('file_id', actualFileId)
  .eq('organization_id', organizationId)
  .limit(1);

if (checkError) {
  console.error('âŒ Erro ao verificar duplicaÃ§Ã£o:', checkError);
  // Continue processing even if check fails
} else if (existingData && existingData.length > 0) {
  if (!forceOverwrite) {
    console.warn('âš ï¸ Arquivo jÃ¡ foi processado anteriormente');
    return new Response(JSON.stringify({
      success: false,
      error: `Arquivo jÃ¡ foi processado em ${new Date(existingData[0].created_at).toLocaleString('pt-BR')}. Use um file_id diferente, adicione "forceOverwrite": true para sobrescrever, ou remova os dados anteriores.`,
      fileId: actualFileId,
      existingRecordId: existingData[0].id
    }), {
      status: 409, // Conflict
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  } else {
    console.log('ğŸ”„ ForÃ§ando sobrescrita - removendo dados anteriores...');
    await supabase
      .from('staging_XX_tabela')
      .delete()
      .eq('file_id', actualFileId)
      .eq('organization_id', organizationId);
  }
}
```

### ğŸ”’ VerificaÃ§Ã£o 2: Merge Keys Duplicadas
```typescript
// Check for duplicate data based on merge keys (optional additional check)
if (processedData.length > 0) {
  console.log(`ğŸ” Verificando dados duplicados por merge keys...`);
  const sampleMergeKeys = processedData.slice(0, 10).map(row => row.merge);

  const { data: duplicateCheck, error: duplicateError } = await supabase
    .from('staging_XX_tabela')
    .select('merge, file_id, created_at')
    .eq('organization_id', organizationId)
    .in('merge', sampleMergeKeys)
    .limit(5);

  if (!duplicateError && duplicateCheck && duplicateCheck.length > 0) {
    if (!forceOverwrite) {
      console.warn(`âš ï¸ Encontradas ${duplicateCheck.length} linhas com dados similares`);
      const oldestDuplicate = duplicateCheck[0];

      return new Response(JSON.stringify({
        success: false,
        error: `Dados similares jÃ¡ existem na base (${duplicateCheck.length} linhas encontradas). Primeiro registro similar foi processado em ${new Date(oldestDuplicate.created_at).toLocaleString('pt-BR')}. Adicione "forceOverwrite": true para processar mesmo assim.`,
        fileId: actualFileId,
        duplicateFileId: oldestDuplicate.file_id,
        duplicateCount: duplicateCheck.length,
        sampleDuplicateKeys: duplicateCheck.map(d => d.merge)
      }), {
        status: 409, // Conflict
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      });
    } else {
      console.log(`ğŸ”„ ForÃ§ando processamento apesar de ${duplicateCheck.length} dados similares existentes`);
    }
  }
}
```

### ğŸ”“ Bypass de ProteÃ§Ãµes
Para forÃ§ar processamento mesmo com duplicaÃ§Ã£o:
```json
{
  "filename": "arquivo.csv",
  "fileId": "uuid-aqui",
  "organizationId": "org-uuid",
  "forceOverwrite": true
}
```

---

## ğŸš¨ PROBLEMAS COMUNS E SOLUÃ‡Ã•ES

### âŒ Problema: "Edge Function returned a non-2xx status code"
âœ… **SoluÃ§Ã£o:** Verificar se arquivo existe no storage com nome correto

### âŒ Problema: "invalid input syntax for type uuid"
âœ… **SoluÃ§Ã£o:** Garantir que fileId seja UUID vÃ¡lido ou gerar com `crypto.randomUUID()`

### âŒ Problema: Dados duplicados na tabela
âœ… **SoluÃ§Ã£o:** Usar as proteÃ§Ãµes implementadas:
- VerificaÃ§Ã£o automÃ¡tica por file_id
- VerificaÃ§Ã£o automÃ¡tica por merge keys
- ParÃ¢metro `forceOverwrite` para bypass controlado

### âŒ Problema: Encoding incorreto (Inclusï¿½o)
âœ… **SoluÃ§Ã£o:** Aplicar replace nos headers:
```typescript
.replace('Inclusï¿½o', 'InclusÃ£o')
.replace('Vagï¿½o', 'VagÃ£o')
```

### âŒ Problema: Status 409 "Arquivo jÃ¡ foi processado"
âœ… **SoluÃ§Ã£o:**
1. Use file_id diferente, ou
2. Adicione `"forceOverwrite": true` no JSON, ou
3. Remova dados anteriores manualmente

---

## ğŸ“‹ CHECKLIST PARA NOVOS PIPELINES

### ğŸ—‚ï¸ Estrutura Base
- [ ] Criar migration com tabela staging_XX
- [ ] Configurar RLS policies
- [ ] Criar edge function process-csv-XX

### ğŸ”§ Funcionalidades ObrigatÃ³rias
- [ ] Implementar parseDate() para datas brasileiras
- [ ] Implementar parseNumericValue() para nÃºmeros brasileiros
- [ ] Adicionar lÃ³gica para cabeÃ§alho na 2Âª linha
- [ ] Adicionar remoÃ§Ã£o de linhas de total
- [ ] Configurar inserÃ§Ã£o em lotes

### ğŸ›¡ï¸ ProteÃ§Ãµes (NOVO!)
- [ ] Adicionar parÃ¢metro `forceOverwrite` na edge function
- [ ] Implementar verificaÃ§Ã£o de file_id duplicado
- [ ] Implementar verificaÃ§Ã£o de merge keys duplicadas
- [ ] Configurar bypass controlado com forceOverwrite

### ğŸ¯ Filtros e CustomizaÃ§Ãµes
- [ ] Implementar filtros especÃ­ficos (se necessÃ¡rio)
- [ ] Personalizar campos merge conforme necessidade
- [ ] Adaptar validaÃ§Ãµes especÃ­ficas do pipeline

### ğŸš€ Deploy e Testes
- [ ] Testar com UUID vÃ¡lido
- [ ] Deploy da edge function
- [ ] Testar proteÃ§Ã£o contra duplicaÃ§Ã£o
- [ ] Testar bypass com forceOverwrite
- [ ] Verificar logs e corrigir erros
- [ ] Documentar especificidades do pipeline

---

## ğŸ¯ RESUMO

**Principais Aprendizados:**
1. CSVs brasileiros precisam de tratamento especial para datas e nÃºmeros
2. CabeÃ§alhos podem estar em linhas diferentes da primeira
3. Ãšltimas linhas frequentemente sÃ£o totais/agrupamentos
4. InserÃ§Ã£o em lote melhora performance
5. ValidaÃ§Ã£o de UUID Ã© crÃ­tica para tabelas Supabase
6. CORS headers sÃ£o essenciais para edge functions
7. **NOVO: ProteÃ§Ãµes contra duplicaÃ§Ã£o sÃ£o essenciais para produÃ§Ã£o**

**Funcionalidades Implementadas:**
âœ… **Pipelines 02, 04 e 05** - Totalmente funcionais
âœ… **Processamento de CSV brasileiro** - Datas, nÃºmeros, encoding
âœ… **ProteÃ§Ãµes contra duplicaÃ§Ã£o** - File ID + Merge Keys + ForceOverwrite
âœ… **Tratamento de erros** - Status codes apropriados (409 para conflitos)
âœ… **Bypass controlado** - ParÃ¢metro forceOverwrite para sobrescrever

**Tempo Economizado:**
- **ImplementaÃ§Ã£o inicial**: ~30min (vs 2-3 horas de debug)
- **ProteÃ§Ã£o contra duplicaÃ§Ã£o**: AutomÃ¡tica - evita reprocessamento desnecessÃ¡rio
- **ManutenÃ§Ã£o**: MÃ­nima - padrÃµes estabelecidos e testados

**Status de ProduÃ§Ã£o:**
ğŸŸ¢ **Pronto para uso** - Ambos pipelines em produÃ§Ã£o com todas as proteÃ§Ãµes