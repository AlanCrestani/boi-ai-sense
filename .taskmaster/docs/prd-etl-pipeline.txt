# PRD - Sistema ETL para Conecta Boi

## Visão Geral
Implementar pipeline ETL robusto para processar arquivos CSV do Supabase Storage, com controle de estado, validação e carga em tabelas dimensionais/fato.

## Contexto
- Sistema já possui edge function process-csv parcialmente implementada
- Arquivos CSV são enviados via upload para Supabase Storage
- Necessidade de rastreabilidade e reprocessamento
- Multi-tenant com RLS em todas as tabelas

## Objetivos
1. Criar pipeline ETL confiável e auditável
2. Implementar controle de estado por arquivo
3. Validar dados antes da carga
4. Permitir reprocessamento sem duplicação
5. Fornecer observabilidade completa

## Escopo - Fase 1 (MVP)
Implementar 2 pipelines prioritários:
- Pipeline 02: Desvio de Carregamento (BAHMAN/SILOKING)
- Pipeline 04: Trato por Curral

## Requisitos Funcionais

### RF1: Estrutura de Controle ETL
- Tabela etl_run para controle de execuções
- Tabela etl_file para manifesto de arquivos
- Tabela etl_run_log para logs detalhados
- Status de arquivo: uploaded → parsed → validated → approved → loaded

### RF2: Descoberta Automática
- Webhook/trigger ao fazer upload no Storage
- Registro automático em etl_file
- Identificação de organization_id pelo path
- Cálculo e armazenamento de checksum

### RF3: Processamento de Arquivos
- Parse com detecção automática de separador
- Mapeamento de headers variados
- Staging com dados brutos (raw_data) e normalizados
- Natural key para idempotência

### RF4: Validações Pipeline 02
- Filtrar apenas BAHMAN e SILOKING
- Validar campos obrigatórios: data, hora, vagão, kg
- Verificar valores negativos impossíveis
- Detectar datas futuras
- Calcular desvios (kg e %)

### RF5: Validações Pipeline 04
- Validar campos: data, hora, curral, trateiro
- Verificar integridade referencial com dim_curral
- Validar quantidades e horários de trato
- Detectar tratamentos duplicados

### RF6: Mapeamento Dimensional
- Resolver curral_codigo → curral_id
- Resolver dieta_nome → dieta_id
- Criar pendências para códigos não encontrados
- Fallback para cadastro manual

### RF7: Carga nas Tabelas Fato
- fato_desvio_carregamento (pipeline 02)
- fato_trato_curral (pipeline 04)
- UPSERT baseado em natural_key
- Preservar source_file_id

### RF8: Interface de Operação
- Lista de arquivos com status
- Botões de ação: Processar, Validar, Aprovar, Carregar
- Timeline de logs por execução
- Filtros por level (INFO, WARN, ERROR, NECESSITA_ACAO)
- Resumo de validações e pendências

### RF9: Reprocessamento
- Detectar reupload por checksum
- Opção forçar reprocessamento
- Limpar staging antes de reprocessar
- Manter histórico em fato via UPSERT

### RF10: Observabilidade
- Dashboard com métricas por pipeline
- Total de arquivos por status
- Taxa de erro por período
- Tempo médio de processamento
- Alertas para falhas recorrentes

## Requisitos Não-Funcionais

### RNF1: Segurança
- RLS em todas as tabelas
- Security definer para operações críticas
- Validação de organization_id em todas operações
- Audit trail completo

### RNF2: Performance
- Índices por organization_id + data_ref
- Processamento streaming para arquivos grandes
- Batch de até 1000 linhas por transação
- Timeout de 30s por arquivo

### RNF3: Confiabilidade
- Transações ACID para cada etapa
- Rollback automático em caso de erro
- Retry com backoff exponencial
- Dead letter queue para falhas persistentes

### RNF4: Escalabilidade
- Preparado para particionamento mensal
- Arquivamento de dados antigos
- Compressão de raw_data após 30 dias

## Fases de Implementação

### Fase 1: Infraestrutura Base (3 dias)
1. Criar tabelas de controle (etl_run, etl_file, etl_run_log)
2. Implementar RLS e políticas
3. Criar webhook de descoberta de arquivos
4. Setup de índices básicos

### Fase 2: Pipeline 02 - Desvio Carregamento (5 dias)
1. Criar etl_staging_02_desvio_carregamento
2. Adaptar edge function process-csv existente
3. Implementar validações específicas
4. Criar fato_desvio_carregamento
5. Testes com arquivos reais

### Fase 3: Pipeline 04 - Trato por Curral (4 dias)
1. Criar etl_staging_04_trato_curral
2. Implementar parse e validações
3. Mapeamento com dim_curral
4. Criar fato_trato_curral
5. Testes integrados

### Fase 4: Interface de Operação (3 dias)
1. Página de gestão ETL
2. Componentes de status e ações
3. Timeline de logs
4. Filtros e busca

### Fase 5: Observabilidade (2 dias)
1. Dashboard de métricas
2. Configurar alertas
3. Integração com Sentry
4. Documentação operacional

## Critérios de Aceitação

### Pipeline 02
- Processa arquivos com 10000+ linhas em < 30s
- Taxa de erro < 1% em dados válidos
- Detecta 100% dos erros de validação conhecidos
- Suporta reprocessamento idempotente

### Pipeline 04
- Mapeia 95%+ dos currais corretamente
- Processa horários de trato corretamente
- Detecta tratamentos duplicados ou inconsistentes
- Preserva dados raw para auditoria

### Interface
- Feedback visual em < 200ms
- Estados claramente identificados por cores
- Logs em português compreensível
- Ações reversíveis com confirmação

## Riscos e Mitigações

### Risco 1: Volume de Dados
- Mitigação: Implementar paginação e lazy loading
- Preparar para particionamento futuro

### Risco 2: Qualidade dos CSVs
- Mitigação: Parser flexível com fallbacks
- Log detalhado de problemas encontrados

### Risco 3: Dimensões Faltantes
- Mitigação: Interface para cadastro rápido
- Importação em lote de novos códigos

### Risco 4: Concorrência
- Mitigação: Locks otimistas por arquivo
- Fila de processamento FIFO

## Métricas de Sucesso
- 90% dos arquivos processados sem intervenção manual
- Tempo médio de processamento < 1 minuto
- Zero duplicação de dados em fatos
- 100% de rastreabilidade (arquivo → staging → fato)
- Redução de 50% no tempo de análise de desvios

## Dependências
- Supabase Storage configurado
- Tabelas dim_curral e dim_dieta populadas
- Edge Functions habilitadas
- Usuários com roles apropriados

## Próximos Passos (Fase 2)
- Pipeline 01: Histórico de Consumo
- Pipeline 03: Desvio de Distribuição
- Pipeline 05: Itens de Trato
- Relatórios automatizados
- API para integrações externas

## Notas de Implementação
- Usar TypeScript para type safety
- Testes unitários para validações
- Documentar mapeamentos de headers
- Criar scripts de manutenção
- Preparar runbook operacional